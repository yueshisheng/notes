### what is  recommend system？

我们先做简单的假设：

1：从大量的信息挑选我们需要的信息

2：用户并没有明确的目标

信息检索用户有明确的目标，这是二者的区别

那我们需要做什么吗？

需要建立一个math model或objective function去评价：how much user will like  an item in a given context？

三个关键字：user、item、context

![image-20200718113219253](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718113219253.png)

### 协同概率模型

是context-free，与上下文无关

用户的显示评价：直接打分

隐式的评价：看的多，说明越好看

交互矩阵：用已出现的值来预测空着的值

![image-20200718113615037](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718113615037.png)

![image-20200718113604838](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718113604838.png)

协同过滤模型其实就是物以类聚，人以群分

基于user：如果我看了还珠格格，而我和我姐姐是一类人，那就给她也推荐还珠格格

基于item：如果我喜欢看还珠格格，还珠格格2和还珠格格一样，那它就会给我推荐还珠格格2

![image-20200718113934148](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718113934148.png)

接下来，咱们计算一下相似度的计算

余弦相似度：如右图的两个向量对应相乘后除以模，这不就是求两个向量的模吗，方向越一样，说明相似度越高呗

pearson相似度：为了修正一下，先减去平均数再求积

![image-20200718114345572](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718114345572.png)



协同过滤的优势：实现简单

缺点：存储空间大，交互矩阵都得存起来，如果只有几次的交互，交互矩阵较为稀疏，相似度计算不准确

### 矩阵分解

是context-free，与上下文无关

要获得更高水平的属性，比如下图三部剧，提取出他们都是漫威出品的属性

![image-20200718115124729](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718115124729.png)

这种更高级的属性通过比较两个对象的embeddeding来实现，embeddeding是一种隐向量，顾名思义，就是隐藏了许多信息的向量

矩阵分解的改进

加各种bias，global bias是给所有的例子都加，item是对特定的item加，user bias是看不同的用户来加

![image-20200718120015015](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718120015015.png)

优点：存储空间小，存embeddeding就可以，不用存交互矩阵

缺点：解释起来麻烦，因为embedding东西太少

### 逻辑回归

包含context信息

把推荐当作classification



![image-20200718150431510](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718150431510.png)

x代表feature vector，就是一些问题的答案，值是0或1.w代表weight

context体现在feature map

![image-20200718150501570](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718150501570.png)

为什么x要用0，1表示

其实就是代表是或否，比较好处理

sigmod函数的值域在0-1，吻合我们的output

优点：model的解释性比较好，因为得出的结果有可能性。速度比较快，训练开销小

缺点：表现依赖于feature，实际应用上得先去处理数据，线性模型，表达能力不行，bias可能比较大

辛普森悖论

在有男女特征时，女生和男生点击第二个电视率更高，但去除特征后，反而第一个点击率更高

说明少选一个特征，可能会对结果有很大的影响

![image-20200718151206077](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718151206077.png)

### 因子分解机

特征交叉：feature crossing

在线性模型加一个交叉项，如下图，就是值为1（有一个值为0，乘起来后，也是0）的特征两两组合，相当于且的关系：

feature 1：他是男的（1）

feature 2：他早上吃早饭（1）

特征交叉是：他是男的且早上吃早饭

![image-20200718151755179](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718151755179.png)

如何计算交叉项的权重呢？

把两个feature的隐向量做点乘



![image-20200718152338119](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718152338119.png)

![image-20200718152516075](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718152516075.png)

优点：good expressive（加入交叉项，特征变丰富了）、good generation、low training cost

缺点：只能是二维的交叉，高维度交叉的代价比较大

### 提升树

gradient boosting decision tree

F代表理想的函数，f代表第一步的拟合函数，f1+f代表拟合更准确的拟合函数，以此后推

![image-20200718152940460](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718152940460.png)

f代表的是回归树，L损失函数代表是与梯度相关的上升决策树--gbdt

xgboost的f不是决策树了，优化目标也有改进

![image-20200718153336092](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718153336092.png)

优点：good expression，不断逼近目标

缺点：并行化难，，样本换了一批，必须重新再训练

上边用LR。来实现分类，得到feature vector

![image-20200718153857659](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718153857659.png)

![image-20200718153953792](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718153953792.png)

### 问题1： 

从基于用户的协同过滤和基于物品的协同过滤的原理思考，下列场景中使用哪种协同过滤算法 更加合适？为什么？

（1）新闻资讯推荐 （2）电商网站推荐

答：新闻推荐更适合用基于用户的协同过滤的算法，而电商网站适应基于物品的协同概率算法。原因如下：

两种协同过滤算法的区别是计算相似度的难度，对于新闻推荐，信息作为item要比用户的数量多的多，所以为了降低计算复杂度，相似度应该是用户打分向量两两点乘，相反，对于，电商网站，用户要比item商品多的多，所以采用基于物品的协同过滤算法

### 问题2：

为什么逻辑回归模型在工业界受到的了广泛应用？LR相对于其他的模型，尤其是GBDT模型， 突出的优点是什么？

答：逻辑回归的解释性较好，因为得出的结果有可能性。速度比较快，训练开销小

相比于GBDT，逻辑回归的训练难度小，能够实现并行化

### 问题3

3：为什么说提升树模型（GBDT）难以并行化？从Boosting方法的原理上给出简单的解释。

答：boosting算法的基本思路是：

1：根据神经网络训练出初始的近似函数F0（让损失函数最小化）

2：下一个近似函数的结果需要新的损失函数来得到，新的损失函数参数为：实际值、F0+f

所以可以看出下一次的近似函数的计算必须有上一次函数的计算结果，所以无法实现并行化

![image-20200718161511297](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718161511297.png)

![image-20200718161529193](C:\Users\yueshisheng\AppData\Roaming\Typora\typora-user-images\image-20200718161529193.png)